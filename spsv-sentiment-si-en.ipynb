{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Libraries, Dependencies and Parameters\n","metadata":{}},{"cell_type":"markdown","source":"_Install Required Libraries_\n","metadata":{"papermill":{"duration":0.007777,"end_time":"2023-05-05T01:29:44.635300","exception":false,"start_time":"2023-05-05T01:29:44.627523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%pip install openprompt\n%pip install evaluate","metadata":{"id":"GJnzG561XzuD","outputId":"8a1c54ef-d5f9-4ac9-9b92-6dff736b9344","papermill":{"duration":21.539989,"end_time":"2023-05-05T01:30:06.182490","exception":false,"start_time":"2023-05-05T01:29:44.642501","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Dependencies_\n","metadata":{"execution":{"iopub.execute_input":"2023-04-19T11:46:38.93501Z","iopub.status.busy":"2023-04-19T11:46:38.934658Z","iopub.status.idle":"2023-04-19T11:46:49.19186Z","shell.execute_reply":"2023-04-19T11:46:49.190506Z","shell.execute_reply.started":"2023-04-19T11:46:38.93497Z"},"papermill":{"duration":0.008942,"end_time":"2023-05-05T01:30:06.200906","exception":false,"start_time":"2023-05-05T01:30:06.191964","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport time\nimport evaluate\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Optional\nfrom collections import defaultdict, namedtuple\nfrom tqdm import tqdm\nfrom datasets import load_dataset, load_metric\nfrom openprompt import PromptDataLoader, PromptForClassification\nfrom openprompt.data_utils import InputExample\nfrom openprompt.data_utils.text_classification_dataset import PROCESSORS\nfrom openprompt.prompts import MixedTemplate, SoftVerbalizer\nfrom openprompt.plms.utils import TokenizerWrapper\nfrom openprompt.utils.logging import logger\nfrom transformers import AdamW, get_linear_schedule_with_warmup, BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n                         XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaForMaskedLM, set_seed\nfrom transformers.models.auto.tokenization_auto import tokenizer_class_from_name\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\nfrom statistics import mode\nfrom yacs.config import CfgNode","metadata":{"id":"6BIRiGiWkVU6","papermill":{"duration":15.053934,"end_time":"2023-05-05T01:30:21.263542","exception":false,"start_time":"2023-05-05T01:30:06.209608","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":0.017063,"end_time":"2023-05-05T01:30:21.290653","exception":false,"start_time":"2023-05-05T01:30:21.273590","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Parameters_\n","metadata":{"papermill":{"duration":0.009005,"end_time":"2023-05-05T01:30:21.308709","exception":false,"start_time":"2023-05-05T01:30:21.299704","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_type = \"XLM-R\"\ntechnique = \"Sentiment\" #@param [\"Sentiment\", \"Humor\", \"Hate-Speech\"]\nover_sampling_technique = \"ROS\"\nsampling_strategy = \"1:0.25:0.25\"\noversample_dataset = False\nuse_cuda = True\nno_of_labels = 4 #[{\"Sentiment\"}:4, {\"Humor\"}:2, {\"Hate-Speech\"}:3]\nvalidation_size = (1/9)\ntest_size = 0.1\nsplit_random_state = 42\nmax_seq_length = 128\nbatch_size = 32\nnum_train_epochs = 20\ntraining_seed = 42 #@param [8, 42, 77]\nscript=\"Char-Script-1.0\"","metadata":{"id":"JUCDlx1akOCt","papermill":{"duration":0.018916,"end_time":"2023-05-05T01:30:21.336912","exception":false,"start_time":"2023-05-05T01:30:21.317996","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preprocessing\n","metadata":{"papermill":{"duration":0.009167,"end_time":"2023-05-05T01:30:21.355402","exception":false,"start_time":"2023-05-05T01:30:21.346235","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sinhala-english-cmcs-dataset/annotated-script(all).csv\")\ndf = df[['Sentence', technique, script]]\ndf.columns = ['Sentence', 'Label', script]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Label Encoding*","metadata":{}},{"cell_type":"code","source":"df['Label'], uniq = pd.factorize(df['Label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Train, Validation & Test Split*","metadata":{}},{"cell_type":"code","source":"X, y = df[['Sentence', script]], df[['Label']]\nstratifying_col = y[\"Label\"]\nX_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=test_size, stratify=stratifying_col, random_state=split_random_state)\nstratifying_col = y_rem[\"Label\"]\nX_train, X_validation, y_train, y_validation = train_test_split(X_rem, y_rem, test_size=validation_size, stratify=stratifying_col, random_state=split_random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df, uniq, X, y, stratifying_col, X_rem, y_rem","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train : Rows =\", X_train.shape[0], \", Columns = \", X_train.shape[1])\nprint(\"y_train : Rows =\", y_train.shape[0], \", Columns = \", y_train.shape[1])\nprint(\"X_validation : Rows =\", X_validation.shape[0], \", Columns = \", X_validation.shape[1])\nprint(\"y_validation : Rows =\", y_validation.shape[0], \", Columns = \", y_validation.shape[1])\nprint(\"X_test : Rows =\", X_test.shape[0], \", Columns = \", X_test.shape[1])\nprint(\"y_test : Rows =\", y_test.shape[0], \", Columns = \", y_test.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Labels :\", ['Negative', 'Neutral', 'Positive', 'Conflict'])\nprint(\"Train :\", y_train.groupby('Label').size().tolist())\nprint(\"Validation :\", y_validation.groupby('Label').size().tolist())\nprint(\"Test :\", y_test.groupby('Label').size().tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Oversampling*","metadata":{}},{"cell_type":"code","source":"def apply_oversampling(x, y):\n  (unique, counts) = np.unique(y, axis=0, return_counts=True)\n  print(\"Class Distribution Without Oversampling\", counts)\n\n  # define oversampling strategy\n  if (over_sampling_technique == \"\"):\n    return x, y\n  elif (over_sampling_technique == \"ROS\"):\n    if (technique==\"Humor\"):\n      oversample = RandomOverSampler(sampling_strategy = float(sampling_strategy))\n    elif (technique==\"Hate-Speech\"):\n      sampling_ratio = sampling_strategy.split(\":\")\n      oversample = RandomOverSampler(sampling_strategy = {\n          0:int(counts[0]*float(sampling_ratio[0])), \n          1:int(counts[0]*float(sampling_ratio[1])), \n          2:int(counts[0]*float(sampling_ratio[2]))\n          })\n    elif (technique==\"Sentiment\"):\n      sampling_ratio = sampling_strategy.split(\":\")\n      oversample = RandomOverSampler(sampling_strategy = {\n          0:int(counts[1]*float(sampling_ratio[0])), \n          1:int(counts[1]*float(sampling_ratio[1])), \n          2:int(counts[1]*float(sampling_ratio[2])),\n          3:int(counts[1]*float(sampling_ratio[3]))\n          })\n  elif (over_sampling_technique == \"ADASYN\"):\n    oversample = ADASYN(sampling_strategy=\"minority\")\n  elif (over_sampling_technique == \"SMOTE\"):\n    oversample = SMOTE()\n  elif (over_sampling_technique == \"BorderlineSMOTE\"):\n    oversample = BorderlineSMOTE()\n\n  # fit and apply the transform\n  X_over, y_over = oversample.fit_resample(x, y)\n\n  (unique, counts) = np.unique(y_over, axis=0, return_counts=True)\n  print(\"Class Distribution After Oversampling\", counts)\n\n  return X_over, y_over","metadata":{"papermill":{"duration":0.021481,"end_time":"2023-05-05T01:30:21.604577","exception":false,"start_time":"2023-05-05T01:30:21.583096","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if oversample_dataset:\n  # apply oversampling\n  X_train = np.array(X_train).reshape(-1, 1)\n  X_train, y_train = apply_oversampling(X_train, y_train)\n  X_train = [x[0] for x in X_train.tolist()]","metadata":{"papermill":{"duration":0.017019,"end_time":"2023-05-05T01:30:21.663004","exception":false,"start_time":"2023-05-05T01:30:21.645985","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OpenPrompt\n","metadata":{}},{"cell_type":"code","source":"set_seed(training_seed)\ntorch.backends.cudnn.deterministic = True ","metadata":{"papermill":{"duration":0.021836,"end_time":"2023-05-05T01:30:21.745360","exception":false,"start_time":"2023-05-05T01:30:21.723524","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLMTokenizerWrapper(TokenizerWrapper):\n    add_input_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n\n    @property\n    def mask_token(self):\n        return self.tokenizer.mask_token\n\n    @property\n    def mask_token_ids(self):\n        return self.tokenizer.mask_token_id\n\n    @property\n    def num_special_tokens_to_add(self):\n        if not hasattr(self, '_num_specials'):\n            self._num_specials = self.tokenizer.num_special_tokens_to_add()\n        return self._num_specials\n\n    def tokenize_one_example(self, wrapped_example, teacher_forcing):\n        ''' # TODO doesn't consider the situation that input has two parts\n        '''\n        wrapped_example, others = wrapped_example\n\n        # for some dataset like SuperGLUE.COPA, the answer requires prediction an span of\n        # the input. Or in generation tasks, we need to generate a piece of target_text.\n        # In these case, it tokenized to the encoded_tgt_text for future use.\n        encoded_tgt_text = []\n        if 'tgt_text' in others:\n            tgt_text = others['tgt_text']\n            if isinstance(tgt_text, str):\n                tgt_text = [tgt_text]\n            for t in tgt_text:\n                encoded_tgt_text.append(self.tokenizer.encode(t, add_special_tokens=False))\n\n        mask_id = 0 # the i-th the mask token in the template.\n\n        encoder_inputs = defaultdict(list)\n        for piece in wrapped_example:\n            if piece['loss_ids']==1:\n                if teacher_forcing: # fill the mask with the tgt task\n                    raise RuntimeError(\"Masked Language Model can't perform teacher forcing training!\")\n                else:\n                    encode_text = [self.mask_token_ids]\n                mask_id += 1\n\n            if piece['text'] in self.special_tokens_maps.keys():\n                to_replace = self.special_tokens_maps[piece['text']]\n                if to_replace is not None:\n                    piece['text'] = to_replace\n                else:\n                    raise KeyError(\"This tokenizer doesn't specify {} token.\".format(piece['text']))\n\n            if 'soft_token_ids' in piece and piece['soft_token_ids']!=0:\n                encode_text = [0] # can be replace by any token, since these token will use their own embeddings\n            else:\n                encode_text = self.tokenizer.encode(piece['text'], add_special_tokens=False)\n\n            encoding_length = len(encode_text)\n            encoder_inputs['input_ids'].append(encode_text)\n            for key in piece:\n                if key not in ['text']:\n                    encoder_inputs[key].append([piece[key]]*encoding_length)\n\n        encoder_inputs = self.truncate(encoder_inputs=encoder_inputs)\n        # delete shortenable ids\n        encoder_inputs.pop(\"shortenable_ids\")\n        encoder_inputs = self.concate_parts(input_dict=encoder_inputs)\n        encoder_inputs = self.add_special_tokens(encoder_inputs=encoder_inputs)\n        # create special input ids\n        encoder_inputs['attention_mask'] = [1] *len(encoder_inputs['input_ids'])\n        if self.create_token_type_ids:\n            encoder_inputs['token_type_ids'] = [0] *len(encoder_inputs['input_ids'])\n        # padding\n        encoder_inputs = self.padding(input_dict=encoder_inputs, max_len=self.max_seq_length, pad_id_for_inputs=self.tokenizer.pad_token_id)\n\n        if len(encoded_tgt_text) > 0:\n            encoder_inputs = {**encoder_inputs, \"encoded_tgt_text\": encoded_tgt_text}# convert defaultdict to dict\n        else:\n            encoder_inputs = {**encoder_inputs}\n        return encoder_inputs","metadata":{"id":"bq3XoGPBdJri","papermill":{"duration":0.027182,"end_time":"2023-05-05T01:30:21.782416","exception":false,"start_time":"2023-05-05T01:30:21.755234","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n\n_MODEL_CLASSES = {\n    'bert': ModelClass(**{\n        'config': BertConfig,\n        'tokenizer': BertTokenizer,\n        'model':BertForMaskedLM,\n        'wrapper': MLMTokenizerWrapper,\n    }),\n    'roberta': ModelClass(**{\n        'config': RobertaConfig,\n        'tokenizer': RobertaTokenizer,\n        'model':RobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n    'xlm': ModelClass(**{\n        'config': XLMRobertaConfig,\n        'tokenizer': XLMRobertaTokenizer,\n        'model': XLMRobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n}\n\ndef get_model_class(plm_type: str):\n    return _MODEL_CLASSES[plm_type]\n\ndef load_plm(model_name, model_path, specials_to_add = None):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`wrapper`: The wrapper class of this plm.\n    \"\"\"\n    model_class = get_model_class(plm_type = model_name)\n    model_config = model_class.config.from_pretrained(model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in model_name: # add pad token for gpt\n        specials_to_add = [\"<pad>\"]\n        # model_config.attn_pdrop = 0.0\n        # model_config.resid_pdrop = 0.0\n        # model_config.embd_pdrop = 0.0\n    model = model_class.model.from_pretrained(model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(model_path)\n    wrapper = model_class.wrapper\n\n\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n\n    if 'opt' in model_name:\n        tokenizer.add_bos_token=False\n    return model, tokenizer, model_config, wrapper\n\ndef load_plm_from_config(config: CfgNode):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`model_config`: The wrapper class of this plm.\n    \"\"\"\n    plm_config = config.plm\n    model_class = get_model_class(plm_type = plm_config.model_name)\n    model_config = model_class.config.from_pretrained(plm_config.model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in plm_config.model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in plm_config.model_name: # add pad token for gpt\n        if \"<pad>\" not in config.plm.specials_to_add:\n            config.plm.specials_to_add.append(\"<pad>\")\n    model = model_class.model.from_pretrained(plm_config.model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(plm_config.model_path)\n    wrapper = model_class.wrapper\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=config.plm.specials_to_add)\n    return model, tokenizer, model_config, wrapper\n\ndef add_special_tokens(model: PreTrainedModel,\n                       tokenizer: PreTrainedTokenizer,\n                       specials_to_add: Optional[List[str]] = None):\n    r\"\"\"add the special_tokens to tokenizer if the special token\n    is not in the tokenizer.\n\n    Args:\n        model (:obj:`PreTrainedModel`): The pretrained model to resize embedding\n                after adding special tokens.\n        tokenizer (:obj:`PreTrainedTokenizer`): The pretrained tokenizer to add special tokens.\n        specials_to_add: (:obj:`List[str]`, optional): The special tokens to be added. Defaults to pad token.\n\n    Returns:\n        The resized model, The tokenizer with the added special tokens.\n\n    \"\"\"\n    if specials_to_add is None:\n        return model, tokenizer\n    for token in specials_to_add:\n        if \"pad\" in token.lower():\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': token})\n                model.resize_token_embeddings(len(tokenizer))\n                logger.info(\"pad token is None, set to id {}\".format(tokenizer.pad_token_id))\n    return model, tokenizer","metadata":{"id":"71QIB8CjdRPI","papermill":{"duration":0.024665,"end_time":"2023-05-05T01:30:21.816834","exception":false,"start_time":"2023-05-05T01:30:21.792169","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Load Pre-trained Language Model (PLM)_\n","metadata":{"papermill":{"duration":0.009676,"end_time":"2023-05-05T01:30:21.836669","exception":false,"start_time":"2023-05-05T01:30:21.826993","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plm, tokenizer, model_config, WrapperClass =load_plm(\"xlm\", \"xlm-roberta-base\")","metadata":{"id":"NDPRMGSXdUwW","outputId":"c322f46e-4506-45f8-84b4-cc3804d9c171","papermill":{"duration":14.223,"end_time":"2023-05-05T01:30:36.069951","exception":false,"start_time":"2023-05-05T01:30:21.846951","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Prompt Template, Verbalizer and PLM_\n","metadata":{"papermill":{"duration":0.010084,"end_time":"2023-05-05T01:30:36.090623","exception":false,"start_time":"2023-05-05T01:30:36.080539","status":"completed"},"tags":[]}},{"cell_type":"code","source":"template = '{\"placeholder\": \"text_a\"}. {\"soft\": \"The\"} {\"soft\": \"sentiment\"} {\"soft\": \"or\"} {\"soft\": \"the\"} {\"soft\": \"feeling\"} {\"soft\": \"of\"} {\"soft\": \"the\"} {\"soft\": \"given\"} {\"soft\": \"sentence\"} {\"soft\": \"can\"} {\"soft\": \"be\"} {\"soft\": \"classified\"} {\"soft\": \"as\"} {\"soft\": \"positive\"} {\"soft\": \",\"} {\"soft\": \"negative\"} {\"soft\": \"or\"} {\"soft\": \"neutral\"} {\"soft\": \".\"} {\"soft\": \"The\"} {\"soft\": \"classified\"} {\"soft\": \"sentiment\"} {\"soft\": \"of\"} {\"soft\": \"the\"} {\"soft\": \"sentence\"} {\"soft\": \"is\"} {\"mask\"}.'\npromptTemplate = MixedTemplate(model=plm, text = template, tokenizer = tokenizer)\npromptVerbalizer = SoftVerbalizer(tokenizer, plm, num_classes=no_of_labels)\npromptModel = PromptForClassification(template = promptTemplate, plm = plm, verbalizer = promptVerbalizer, freeze_plm=True)","metadata":{"id":"jD4_Q5w0dXqx","papermill":{"duration":0.027731,"end_time":"2023-05-05T01:30:36.128590","exception":false,"start_time":"2023-05-05T01:30:36.100859","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del plm, model_config, template, promptVerbalizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoaders\n","metadata":{}},{"cell_type":"code","source":"X_train, y_train = X_train.values.tolist(), y_train.values.tolist()\nX_validation, y_validation = X_validation.values.tolist(), y_validation.values.tolist()\nX_test, y_test = X_test.values.tolist(), y_test.values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Train Dataloader_\n","metadata":{"papermill":{"duration":0.010913,"end_time":"2023-05-05T01:30:36.712596","exception":false,"start_time":"2023-05-05T01:30:36.701683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_examples=[]\n\nfor i in range(len(X_train)):\n  train_examples.append(InputExample(guid = i, text_a = X_train[i][0], label = y_train[i][0]))\n\ntrain_data_loader = PromptDataLoader(\n    dataset = train_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\"\n).dataloader","metadata":{"papermill":{"duration":23.672375,"end_time":"2023-05-05T01:31:00.427569","exception":false,"start_time":"2023-05-05T01:30:36.755194","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Validation Dataloader_\n","metadata":{}},{"cell_type":"code","source":"validation_examples=[]\n\nfor i in range(len(X_validation)):\n    validation_examples.append(InputExample(guid = i, text_a = X_validation[i][0], label = y_validation[i][0]))\n\nvalidation_data_loader = PromptDataLoader(\n    dataset = validation_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\"\n).dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Test Dataloader_\n","metadata":{"papermill":{"duration":0.022549,"end_time":"2023-05-05T01:31:00.473485","exception":false,"start_time":"2023-05-05T01:31:00.450936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_examples=[]\ntest_examples_latin=[]\ntest_examples_sinhala=[]\ntest_examples_mixed=[]\n\nfor i in range(len(X_test)):\n  test_examples.append(InputExample(guid = i, text_a = X_test[i][0], label = y_test[i][0]))\n\n  if(X_test[i][1]==\"Latin\"):\n      test_examples_latin.append(InputExample(guid = i, text_a = X_test[i][0], label = y_test[i][0]))\n  elif(X_test[i][1]==\"Sinhala\"):\n      test_examples_sinhala.append(InputExample(guid = i, text_a = X_test[i][0], label = y_test[i][0]))    \n  elif(X_test[i][1]==\"Mixed\"):\n      test_examples_mixed.append(InputExample(guid = i, text_a = X_test[i][0], label = y_test[i][0]))\n\ntest_data_loader = PromptDataLoader(\n    dataset = test_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\"\n).dataloader\ntest_data_loader_latin = PromptDataLoader(\n    dataset = test_examples_latin,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\"\n).dataloader\ntest_data_loader_sinhala = PromptDataLoader(\n    dataset = test_examples_sinhala,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\"\n).dataloader\ntest_data_loader_mixed = PromptDataLoader(\n    dataset = test_examples_mixed,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=batch_size,\n    max_seq_length=max_seq_length,\n    truncation=True,\n    padding=\"max_length\",\n).dataloader","metadata":{"papermill":{"duration":3.108347,"end_time":"2023-05-05T01:31:03.656862","exception":false,"start_time":"2023-05-05T01:31:00.548515","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tokenizer, WrapperClass, promptTemplate, X_train, y_train, X_validation, y_validation, X_test, y_test, train_examples, validation_examples, test_examples, test_examples_latin, test_examples_sinhala, test_examples_mixed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Optimizer and Scheduler\n","metadata":{}},{"cell_type":"code","source":"no_decay = ['bias', 'LayerNorm.weight']\nnum_training_steps = len(train_data_loader) * num_train_epochs # After the num_training_steps lr will be 0\n\nbetas = (0.9, 0.999)\neps = 1e-08\nnum_warmup_steps = 1250\nsp_lr = 1e-2\nsv_gp_1_lr = 2e-2\nsv_gp_2_lr = 2e-2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sp_optimizer_grouped_parameters = [{'params': [p for n,p in promptModel.template.named_parameters() if \"raw_embedding\" not in n]}]\nsp_optimizer = AdamW(sp_optimizer_grouped_parameters, lr=sp_lr, betas=betas, eps=eps)\nsp_scheduler = get_linear_schedule_with_warmup(sp_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n\nsv_optimizer_grouped_parameters = [{'params': promptModel.verbalizer.group_parameters_1, \"lr\":sv_gp_1_lr}, {'params': promptModel.verbalizer.group_parameters_2, \"lr\":sv_gp_2_lr}]\nsv_optimizer = AdamW(sv_optimizer_grouped_parameters,betas=betas,eps=eps)\nsv_scheduler = get_linear_schedule_with_warmup(sv_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)","metadata":{"id":"_bid98egnL2r","outputId":"62c67d59-283e-48f1-db58-1bc46cc46b84","papermill":{"duration":0.038702,"end_time":"2023-05-05T01:31:03.721532","exception":false,"start_time":"2023-05-05T01:31:03.682830","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Evaluation Methods\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(allpreds,alllabels):\n    metric1 = load_metric(\"precision\")\n    metric2 = load_metric(\"recall\")\n    metric3 = load_metric(\"f1\")\n    metric4 = load_metric(\"accuracy\")\n    \n    predictions, labels = allpreds,alllabels\n    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    macro_precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n    macro_recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n    macro_f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n    return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"macro_precision\": macro_precision, \"macro_recall\": macro_recall, \"macro_f1\": macro_f1}","metadata":{"papermill":{"duration":0.036044,"end_time":"2023-05-05T01:31:03.782101","exception":false,"start_time":"2023-05-05T01:31:03.746057","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(promptModel, dataloader):\n    promptModel.eval()\n    allpreds = []\n    alllabels = []\n\n    for step, inputs in enumerate(dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = promptModel(inputs)\n        labels = inputs['label']\n        alllabels.extend(labels.cpu().tolist())\n        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n    return compute_metrics(allpreds,alllabels)","metadata":{"papermill":{"duration":0.032776,"end_time":"2023-05-05T01:31:03.838998","exception":false,"start_time":"2023-05-05T01:31:03.806222","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_loss_and_f1(promptModel, dataloader):\n    promptModel.eval()\n    allpreds = []\n    alllabels = []\n    total_loss = 0\n\n    for step, inputs in enumerate(dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = promptModel(inputs)\n        labels = inputs['label']\n        loss = (torch.nn.CrossEntropyLoss())(logits, labels)\n        total_loss += loss.item()\n        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n        alllabels.extend(labels.cpu().tolist())\n    \n    macro_f1 = load_metric(\"f1\").compute(predictions=allpreds, references=alllabels, average=\"macro\")[\"f1\"]\n    return macro_f1, (total_loss/len(dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train\n","metadata":{}},{"cell_type":"markdown","source":"_Training Parameters_\n","metadata":{"papermill":{"duration":0.022855,"end_time":"2023-05-05T01:31:09.173793","exception":false,"start_time":"2023-05-05T01:31:09.150938","status":"completed"},"tags":[]}},{"cell_type":"code","source":"loss_func = torch.nn.CrossEntropyLoss()\ntot_loss = 0\nlog_loss = 0\nbest_val_acc = 0\n\ntot_train_time = 0\npbar_update_freq = 10\n\nglb_step = 0\nactual_step = 0\nmax_grad_norm = 1.0\neval_every_steps = 100\ngradient_accumulation_steps = 1\n\nleave_training = False\nval_metric = \"macro_f1\"\n\nbest_epoch = -1\nearly_stop_epoch_thresh = 5\n\nepoch_traces = []\nacc_traces = []\nvalidation_loss_traces = []","metadata":{"papermill":{"duration":0.031245,"end_time":"2023-05-05T01:31:09.227894","exception":false,"start_time":"2023-05-05T01:31:09.196649","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Train the Model_\n","metadata":{"papermill":{"duration":0.025293,"end_time":"2023-05-05T01:31:09.277335","exception":false,"start_time":"2023-05-05T01:31:09.252042","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if use_cuda:\n    promptModel=promptModel.cuda()\n\nset_seed(training_seed)","metadata":{"papermill":{"duration":5.263989,"end_time":"2023-05-05T01:31:09.127029","exception":false,"start_time":"2023-05-05T01:31:03.863040","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pbar = tqdm(total=num_training_steps, desc=\"Train\")\nfor epoch in range(num_train_epochs):\n    print(f\"Begin Epoch {epoch}\")\n    epoch_start_time = time.time()\n    for step, inputs in enumerate(train_data_loader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = promptModel(inputs)\n        labels = inputs['label']\n        loss = loss_func(logits, labels)\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n        tot_loss += loss.item()\n        actual_step += 1\n\n        if actual_step % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(promptModel.parameters(), max_grad_norm)\n            glb_step += 1\n            \n            if glb_step % pbar_update_freq == 0:              \n                aveloss = (tot_loss - log_loss)/pbar_update_freq\n                pbar.update(pbar_update_freq)\n                pbar.set_postfix({'Average Loss': aveloss, \"Epoch\": epoch})\n                log_loss = tot_loss\n\n            if sp_optimizer is not None:\n                sp_optimizer.step()\n                sp_optimizer.zero_grad()\n            if sp_scheduler is not None:\n                sp_scheduler.step()\n            if sv_optimizer is not None:\n                sv_optimizer.step()\n                sv_optimizer.zero_grad()\n            if sv_scheduler is not None:\n                sv_scheduler.step()\n\n        if glb_step > num_training_steps:\n            leave_training = True\n            break\n    \n    val_acc, val_loss = calculate_loss_and_f1(promptModel, validation_data_loader)\n    epoch_traces.append(epoch)\n    acc_traces.append(val_acc)\n    validation_loss_traces.append(val_loss)\n    print(\"Validation: [Epoch: {}, Macro F1: {}, Validation Loss: {}, Time per Epoch: {}]\".format(epoch, val_acc, val_loss, time.time()-epoch_start_time), flush=True)\n\n    if val_acc > best_val_acc:\n        torch.save(promptModel.state_dict(),f\"best_model.ckpt\")\n        best_val_acc = val_acc\n        best_epoch = epoch\n        \n    elif (epoch - best_epoch) >= early_stop_epoch_thresh:\n        print(\"Training stopped early at Epoch: %d\" % epoch)\n        break  # Terminate the training loop\n\n    if leave_training:\n        break","metadata":{"papermill":{"duration":2699.355783,"end_time":"2023-05-05T02:16:08.657622","exception":false,"start_time":"2023-05-05T01:31:09.301839","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation\n","metadata":{}},{"cell_type":"markdown","source":"_Load Best Model_\n","metadata":{"papermill":{"duration":0.070067,"end_time":"2023-05-05T02:16:08.799765","exception":false,"start_time":"2023-05-05T02:16:08.729698","status":"completed"},"tags":[]}},{"cell_type":"code","source":"promptModel.load_state_dict(torch.load(f\"best_model.ckpt\"))\nif use_cuda:\n    promptModel = promptModel.cuda()","metadata":{"id":"RA4T07_wowtR","outputId":"376d90c8-054d-4aed-b126-d28c50009156","papermill":{"duration":0.990805,"end_time":"2023-05-05T02:16:09.859369","exception":false,"start_time":"2023-05-05T02:16:08.868564","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Validate the Model_\n","metadata":{"papermill":{"duration":0.070253,"end_time":"2023-05-05T02:16:10.057509","exception":false,"start_time":"2023-05-05T02:16:09.987256","status":"completed"},"tags":[]}},{"cell_type":"code","source":"evaluate(promptModel, validation_data_loader)","metadata":{"papermill":{"duration":14.448975,"end_time":"2023-05-05T02:16:24.578430","exception":false,"start_time":"2023-05-05T02:16:10.129455","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Plot Validation Graphs*","metadata":{}},{"cell_type":"code","source":"plt.plot(range(len(epoch_traces)), acc_traces)\nplt.xlabel('Epoch')\nplt.ylabel('Macro F1-Score')\nplt.title('Epoch vs Validation Macro F1-Score')\nplt.xticks(range(len(epoch_traces)), epoch_traces)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(len(epoch_traces)), validation_loss_traces)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Epoch vs Validation Loss')\nplt.xticks(range(len(epoch_traces)), epoch_traces)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test\n","metadata":{}},{"cell_type":"markdown","source":"**Overall Evaluation**\n","metadata":{}},{"cell_type":"code","source":"evaluate(promptModel, test_data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Script-Wise Evaluation**\n","metadata":{"papermill":{"duration":0.069187,"end_time":"2023-05-05T02:16:24.720984","exception":false,"start_time":"2023-05-05T02:16:24.651797","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"_Latin Script_\n","metadata":{"papermill":{"duration":0.070521,"end_time":"2023-05-05T02:16:24.861646","exception":false,"start_time":"2023-05-05T02:16:24.791125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"evaluate(promptModel, test_data_loader_latin)","metadata":{"papermill":{"duration":10.542221,"end_time":"2023-05-05T02:16:35.470963","exception":false,"start_time":"2023-05-05T02:16:24.928742","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Sinhala Script_\n","metadata":{"papermill":{"duration":0.069335,"end_time":"2023-05-05T02:16:35.617583","exception":false,"start_time":"2023-05-05T02:16:35.548248","status":"completed"},"tags":[]}},{"cell_type":"code","source":"evaluate(promptModel, test_data_loader_sinhala)","metadata":{"papermill":{"duration":2.80025,"end_time":"2023-05-05T02:16:38.488359","exception":false,"start_time":"2023-05-05T02:16:35.688109","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Mixed Script_\n","metadata":{"papermill":{"duration":0.072423,"end_time":"2023-05-05T02:16:38.635769","exception":false,"start_time":"2023-05-05T02:16:38.563346","status":"completed"},"tags":[]}},{"cell_type":"code","source":"evaluate(promptModel, test_data_loader_mixed)","metadata":{"papermill":{"duration":2.774075,"end_time":"2023-05-05T02:16:41.482163","exception":false,"start_time":"2023-05-05T02:16:38.708088","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}